from pathlib import Path

import numpy as np
import pandas as pd
import spikeextractors as se


def get_sorting_info(
    ks_dir
):
    # Read params.py
    d = {}
    with open(ks_dir/"params.py") as f:
        for line in f.readlines():
            (key, val) = line.rstrip('\n').split(' = ')
            d[key] = val
    d['sample_rate'] = float(d['sample_rate'])
    d['n_channels_dat'] = int(d['n_channels_dat'])
    d['dtype'] = str(d['dtype'].strip("'"))
    d['hp_filtered'] = bool(d['hp_filtered'])
    # duration
    tmp_extr = se.BinDatRecordingExtractor(
        ks_dir/'temp_wh.dat',
        d['sample_rate'],
        d['n_channels_dat'],
        d['dtype'],
    )
    d['duration'] = tmp_extr.get_num_frames() / tmp_extr.get_sampling_frequency()
    return d


def load_sorting_extractor(
    ks_dir, good_only=False, drop_noise=True, depth_interval=None
):
    # Extractor
    extr = se.KiloSortSortingExtractor(ks_dir)
    print(f"N clusters (all)={len(extr.get_unit_ids())}")
    return subset_clusters(
        extr,
        get_cluster_info(ks_dir),
        good_only=good_only,
        drop_noise=drop_noise,
        depth_interval=depth_interval,
    )


def get_cluster_info(ks_dir):
    """Load `cluster_info.tsv` generated by phy."""
    # Load cluster info
    cluster_info_path = ks_dir / "cluster_info.tsv"
    if not cluster_info_path.exists():
        import warnings
        warnings.warn("No `cluster_info.tsv` file in ks dir. Generating it using phy.")
        create_phy_cluster_info(ks_dir)
    info = pd.read_csv(cluster_info_path, sep="\t")
    return info


def create_phy_cluster_info(ks_dir):
    """Create `cluster_info.tsv` in kilosort dir.
    
    Same effect as running `phy template-gui <ks_dir>/params.py` and hitting save
    in the GUI."""
    from phy.apps.template import TemplateController
    from phylib.io.model import load_model
    model = load_model(Path(ks_dir)/'params.py')
    controller = TemplateController(
        dir_path=ks_dir,
        model=model
    )
    controller._save_cluster_info()



def get_cluster_groups(ks_dir):
    """Load cluster group as (nclust, 0) np array.

    Use KSLabel assignment when curated `group` is None.
    """
    info = get_cluster_info(ks_dir)
    kslabel = info["KSLabel"]
    curated_group = info["group"]
    return _get_cluster_groups(kslabel, curated_group)


def _get_cluster_groups(kslabel, curated_group):
    assert len(kslabel) == len(curated_group)
    use_KSLabel = pd.isna(curated_group).values
    use_KSLabel[0:3] = False
    group = np.empty((len(kslabel),), dtype=object)
    group[use_KSLabel] = kslabel[use_KSLabel]
    group[~use_KSLabel] = curated_group[~use_KSLabel]
    group[np.where(group == 'nan')[0]] = np.nan
    return group


def subset_clusters(
    extractor, info, good_only=False, drop_noise=True, depth_interval=None
):
    """
    Subset a spikeinterface extractor object.

    Args:
        extractor (spikeextractors.sortingextractor)
        info (pd.DataFrame): As loaded from `cluster_info.tsv`.
    """
    for col in ["cluster_id", "KSLabel", "group"]:
        assert col in info.columns
    assert len(info) == len(extractor.get_unit_ids())
    assert set(info["cluster_id"]) == set(extractor.get_unit_ids())

    print(
        f"Subset clusters: good_only={good_only}, "
        f"drop_noise={drop_noise}, depths={depth_interval}"
    )

    # Group taking in account manual curation or reverting to automatic KS label otherwise
    curated_group = _get_cluster_groups(info["KSLabel"], info["group"])

    n_clusters = len(info)
    keep_cluster = np.ones((n_clusters,), dtype=bool)
    if drop_noise:
        not_noise_rows = curated_group != "noise"
        print(f"Drop N={len(np.where(~not_noise_rows)[0])}/{n_clusters} noise clusters")
        keep_cluster = keep_cluster & not_noise_rows

    if good_only:
        good_rows = curated_group == "good"
        print(
            f"Drop N={len(np.where(~good_rows)[0])}/{n_clusters} not 'good' clusters"
        )
        keep_cluster = keep_cluster & good_rows

    if depth_interval is not None:
        depth_rows = info["depth"].between(*depth_interval)
        print(f"Drop N={len(np.where(~depth_rows)[0])}/{n_clusters} not within depth")
        keep_cluster = keep_cluster & depth_rows
    
    info_subset = info.loc[keep_cluster]
    print(f"Subselect N = {len(info_subset)}/{n_clusters} clusters", end='')
    subextractor = se.SubSortingExtractor(
        extractor, unit_ids=list(info_subset["cluster_id"])
    )
    assert set(info_subset["cluster_id"]) == set(subextractor.get_unit_ids())
    assert len(info_subset["cluster_id"]) == len(subextractor.get_unit_ids())
    print('done')

    return subextractor
